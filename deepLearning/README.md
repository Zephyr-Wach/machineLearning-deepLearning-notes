# 深度学习

[上一章](../machineLearning/conclusion.md) --- [下一章](./deepNeuralNetwork-1.md)

---

## 深度学习基础

**通俗解释**: 深层神经网络学习高级表示。重点：大数据+计算驱动。

## 前言

**通俗解释**: 深度学习变革AI。重点：从浅层到深层。

## 为什么深度学习在近几年流行起来

**通俗解释**: 数据激增、GPU进步、算法优化。重点：规模效应。

## 深度学习先驱：Jeff Hinton访谈

**通俗解释**: Hinton推动反向传播。重点：信念驱动创新。

## 神经网络的编程基础

**通俗解释**: 用Python/Numpy实现网络。重点：向量化加速。

## 二分类问题

**通俗解释**: 预测0/1概率。重点：逻辑回归基础。

## 符号定义

**通俗解释**: X输入，Y标签，W权重。重点：矩阵表示。

## 逻辑回归

**通俗解释**: 用Sigmoid预测概率。重点：二分类起点。

## 向量化 Vectorization

**通俗解释**: 用矩阵运算代替循环。重点：速度提升10倍+。

## 向量化的更多例子

**通俗解释**: 逻辑回归向量化实现。重点：np.dot等。

## Python 中的广播

**通俗解释**: 自动扩展维度运算。重点：简化代码。

## 关于 Python Numpy 向量的说明

**通俗解释**: 注意行列向量。重点：用reshape明确。

## Jupyter/iPython Notebooks快速入门

**通俗解释**: 交互式开发环境。重点：实验友好。

## 深度学习先驱：Pieter Abbeel访谈

**通俗解释**: Abbeel专注强化学习。重点：机器人应用。

## 浅层神经网络

**通俗解释**: 一个隐藏层网络。重点：引入非线性。

## 神经网络概述

**通俗解释**: 输入-隐藏-输出结构。重点：层级抽象。

## 神经网络的表示

**通俗解释**: A^{[l]}表示激活。重点：跟踪维度。

## 计算神经网络的输出

**通俗解释**: 逐层前向计算。重点：选激活函数。

## 神经网络的计算

**通俗解释**: Z = WA + b。重点：矩阵乘法。

## 向量化计算

**通俗解释**: 全样本矩阵运算。重点：并行处理。

## 多样本向量化

**通俗解释**: X (n,m)批处理。重点：高效。

## 向量化实现的解释

**通俗解释**: 矩阵运算替代循环。重点：提升效率。

## 激活函数

**通俗解释**: ReLU快，Sigmoid易饱和。重点：选对防梯度消失。

## 为什么需要非线性激活函数？

**通俗解释**: 无非线性=线性模型。重点：捕捉复杂模式。

## 激活函数的导数

**通俗解释**: 用于反向传播。重点：ReLU导数简单(1/0)。

## 神经网络的梯度下降

**通俗解释**: 更新W,b优化损失。重点：调学习率α。

## 直观理解反向传播

**通俗解释**: 误差逆推调整权重。重点：链式法则。

## 随机初始化

**通俗解释**: 小随机值破对称。重点：如Xavier初始化。

## 深度学习先驱：Ian Goodfellow访谈

**通俗解释**: Goodfellow发明GAN。重点：生成模型。

## 深层神经网络

**通俗解释**: 多隐藏层捕捉高级特征。重点：深度优势。

## 深层神经网络

**通俗解释**: L层网络。重点：复杂任务适用。

## 前向传播和反向传播

**通俗解释**: 前向算A，反向算dA。重点：缓存Z。

## 深层网络中的前向传播

**通俗解释**: 循环l=1到L计算。重点：维度一致。

## 核对矩阵的维数

**通俗解释**: W^{[l]} (n^{[l]}, n^{[l-1]})。重点：调试关键。

## 为什么使用深层表示？

**通俗解释**: 层级组合特征，如脸=眼+鼻。重点：指数效率。

## 搭建神经网络块

**通俗解释**: 模块化函数实现。重点：易扩展。

## 参数VS超参数

**通俗解释**: 参数学得，超参调优。重点：超参影响大。

## 深度学习和大脑的关联性

**通俗解释**: 灵感来自大脑，但简化。重点：并行计算。

## 项目练习

**通俗解释**: 实现浅/深网络。重点：实践巩固。