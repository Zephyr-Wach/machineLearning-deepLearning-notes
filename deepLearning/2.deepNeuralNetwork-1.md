# 深度神经网络调参和优化（1）

## 深度学习的实践层面

[上一章](1.README.md) --- [下一章](2.deepNeuralNetwork-2.md)

---

**通俗解释**: 调参提升性能。重点：系统性方法。

## 训练集、验证集、测试集

**通俗解释**: 训练学模型，验证调参，测试评估。重点：70/20/10划分。

## 应用型机器学习是快速迭代的过程

**通俗解释**: 快速原型，迭代改进。重点：MVP思维。

## 训练集、验证集、测试集

**通俗解释**: 确保无偏评估。重点：随机划分。

## 不一致的训练集和测试机分布

**通俗解释**: 训练广，测试目标化。重点：验证集匹配测试。

## 偏差、方差 (Bias / Variance)

**通俗解释**: 训练误差高=偏差，验证误差高=方差。重点：诊断优化。

## 机器学习基础

**通俗解释**: 偏差欠拟合，方差过拟合。重点：平衡二者。

## 正则化（Regularization）

**通俗解释**: 加惩罚项防过拟合。重点：L2最常用。

**数学公式**:
$$
J(\theta) = J(\theta)_{\text{原}} + \frac{\lambda}{2} \sum \theta_j^2
$$

## 逻辑回归中的正则化

**通俗解释**: 在逻辑回归加L2项。重点：控制模型复杂。

## 神经网络中的正则化

**通俗解释**: 在神经网络加L2或L1。重点：减小权重。

## 为什么正则化有利于预防过拟合？

**通俗解释**: 限制权重，简化模型。重点：提高泛化。

## Dropout 正则化

**通俗解释**: 随机丢弃神经元。重点：防止依赖单一路径。

## 理解 Dropout

**通俗解释**: 像集成学习，增强鲁棒。重点：验证时不丢。

## 其他正则化方法

**通俗解释**: 数据增强、早停。重点：多样防过拟合。

## 归一化输入

**通俗解释**: 标准化特征加速训练。重点：均值0，方差1。

## 梯度消失/梯度爆炸

**通俗解释**: 深层网络梯度过小/大。重点：影响收敛。

## 神经网络的权重初始化

**通俗解释**: Xavier/He初始化防消失。重点：适配激活函数。

## 梯度的数值逼近

**通俗解释**: 数值法验证梯度。重点：慢但可靠。

## 梯度检验

**通俗解释**: 检查反向传播实现。重点：调试工具。

## 梯度检验应用的注意事项

**通俗解释**: 关正则化，ε小。重点：避免误差积累。

## 项目练习

**通俗解释**: 实现正则化、初始化。重点：实践调参。