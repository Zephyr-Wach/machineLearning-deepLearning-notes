# 序列模型（1）

[上一章](4.convolutionalNeuralNetwork-4.md) --- [下一章](5.sequenceModel-2.md)

---

## 循环神经网络 (RNN - Recurrent Neural Networks)

**通俗解释**: RNN处理序列数据，适合时间序列。重点：记忆能力。

## 为什么学序列模型？

**通俗解释**: 处理时间/顺序相关数据。重点：如语音、文本。

## 数学符号

**通俗解释**: x<t>, y<t>表示序列。重点：时间步。

## RNN模型

**通俗解释**: 循环传递隐藏状态。重点：共享参数。

**数学公式**:
$$
h_t = g(W_h h_{t-1} + W_x x_t + b)
$$

## 通过时间的反向传播

**通俗解释**: 沿时间步反传梯度。重点：计算复杂。

## 不同类型的RNN

**通俗解释**: 一对一、一对多、多对一等。重点：任务适配。

## 语言模型和序列生成

**通俗解释**: 预测下一个词生成序列。重点：概率建模。

## 对新序列采样

**通俗解释**: 按概率随机生成。重点：创造性输出。

## 循环神经网络的梯度消失

**通俗解释**: 长序列梯度变小。重点：影响学习。

## GRU单元

**通俗解释**: 门控循环单元，简化LSTM。重点：防梯度消失。

## 长短期记忆 LSTM

**通俗解释**: 长记忆单元，捕捉长期依赖。重点：更复杂。

## 双向循环神经网络

**通俗解释**: 前后双向处理序列。重点：上下文增强。

## Deep RNNs

**通俗解释**: 多层RNN增加容量。重点：计算代价高。