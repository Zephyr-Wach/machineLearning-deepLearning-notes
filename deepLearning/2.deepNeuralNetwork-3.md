# 深度神经网络调参和优化（3）

## 超参数调试、Batch正则化和程序框架

[上一章](2.deepNeuralNetwork-2.md) --- [下一章](3.mlStrategy-1.md)

---

## 超参数调试优先级

**通俗解释**: 优先调学习率、层数等。重点：影响大。

## 超参数取值范围

**通俗解释**: 随机采样对数范围。重点：覆盖广。

## 超参数调试实践

**通俗解释**: 网格搜索或随机搜索。重点：迭代优化。

## 激活函数归一化 (Batch Norm)

**通俗解释**: 每层标准化激活值。重点：加速训练。

## Batch Norm 在神经网络中的实现

**通俗解释**: 均值方差归一化。重点：加可学习参数。

**数学公式**:
$$
\hat{x} = \frac{x - \mu}{\sqrt{\sigma^2 + \epsilon}}, \quad y = \gamma \hat{x} + \beta
$$

## Batch Norm为什么奏效

**通俗解释**: 稳定内部协变量。重点：泛化更好。

## 测试时的Batch Norm

**通俗解释**: 用训练均值方差。重点：一致性。

## Softmax 回归

**通俗解释**: 多分类概率输出。重点：归一化指数。

**数学公式**:
$$
\hat{y}_k = \frac{e^{z_k}}{\sum e^{z_j}}
$$

## 训练Softmax 分类器

**通俗解释**: 交叉熵损失优化。重点：多类任务。

## 深度学习框架

**通俗解释**: TensorFlow/PyTorch简化开发。重点：快速原型。

## TensorFlow

**通俗解释**: 工业级DL框架。重点：灵活与高效。

## 项目练习

**通俗解释**: 用框架实现网络。重点：实践框架。