# 深度神经网络调参和优化（2）

## 优化算法

[上一章](2.deepNeuralNetwork-1.md) --- [下一章](2.deepNeuralNetwork-3.md)

---

**通俗解释**: 改进梯度下降，加速收敛。重点：效率提升。

## Mini-Batch 梯度下降

**通俗解释**: 小批量更新权重。重点：平衡速度与稳定。

## Mini-Batch梯度下降法算法

**通俗解释**: 每批计算梯度。重点：随机打乱数据。

## 理解Mini-Batch梯度下降法

**通俗解释**: 噪声大但快。重点：批大小影响。

## 选择Mini-Batch Size

**通俗解释**: 64/128/256常见。重点：硬件适配。

## Mini-Batch Size的指导原则

**通俗解释**: 小批快，大批稳。重点：实验调优。

## 指数加权平均数

**通俗解释**: 平滑梯度，考虑历史。重点：动量基础。

**数学公式**:
$$
v_t = \beta v_{t-1} + (1-\beta) \theta_t
$$

## 理解指数加权平均数

**通俗解释**: 像滑动窗口平均。重点：β控制历史权重。

## 指数加权平均的偏差修正

**通俗解释**: 初始偏差校正。重点：早期更准。

## Momentum动量梯度下降法

**通俗解释**: 用动量加速梯度。重点：平滑路径。

## RMSprop

**通俗解释**: 自适应学习率。重点：稳定收敛。

## Adam

**通俗解释**: 结合动量和RMSprop。重点：默认优选。

## 学习率衰减

**通俗解释**: 随时间减小学习率。重点：后期精细。

## 局部最优的问题

**通俗解释**: 现代网络少局部最优。重点：鞍点更常见。

## 项目练习

**通俗解释**: 实现Adam优化。重点：调参实践。