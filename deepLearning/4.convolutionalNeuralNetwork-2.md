# 卷积神经网络（2）

## 深度卷积网络：实例探究

[上一章](4.convolutionalNeuralNetwork-1.md) --- [下一章](4.convolutionalNeuralNetwork-3.md)

---

## 为什么要进行实例探究？

**通俗解释**: 经典网络启发设计。重点：学成功经验。

## 经典网络

**通俗解释**: LeNet、AlexNet、VGG等奠基。重点：历史演进。

## LeNet-5

**通俗解释**: 早期CNN，简单有效。重点：手写数字。

## AlexNet

**通俗解释**: 深层CNN，引入ReLU/Dropout。重点：ImageNet突破。

## VGG-16

**通俗解释**: 深层规则网络。重点：简单但参数多。

## 残差网络

**通俗解释**: ResNet用残差连接防退化。重点：深层可训练。

## 残差网络为什么有效

**通俗解释**: 跳跃连接保留信息。重点：梯度流动。

## 网络中的网络以及 1×1 卷积

**通俗解释**: 1×1卷积降维或加非线性。重点：高效。

## Inception 网络

**通俗解释**: 多核并行，减少参数。重点：宽网络。

## 构建Inception 网络

**通俗解释**: 组合多种卷积核。重点：模块化。

## 使用开源的实现方案

**通俗解释**: 用预训练模型。重点：快速开发。

## 迁移学习

**通俗解释**: 微调预训练CNN。重点：小数据适用。

## 数据增强

**通俗解释**: 翻转、裁剪增加数据。重点：防过拟合。

## CV现状

**通俗解释**: CNN主导视觉任务。重点：持续创新。