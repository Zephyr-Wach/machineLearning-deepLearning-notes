# 神经网络

[上一章](./logisticRegression.md) --- [下一章](./systemDesign.md)

---

## 背景介绍

1. 背景 & 动机

### 为什么需要神经网络:
逻辑回归可以解决二分类，但遇到复杂问题（非线性边界、图像识别、语音识别），逻辑回归力不从心。
神经网络能自动学习复杂的非线性映射。

### 神经元和大脑类比:
- 生物神经元有输入（树突）、处理（胞体）、输出（轴突）。
- 人工神经元模仿它：加权求和 → 激活函数 → 输出。

---

## 模型表示

在逻辑回归中，本质上是画一条直线（高维时是超平面），来把 正类 vs 负类 分开。
所以逻辑回归只能直接做 二分类 或者通过 一对多 组合做多分类。
👉 它的本质还是：一次判断 YES / NO。

而神经网络是在逻辑回归的基础上更加深入，
它对比逻辑回归就是，从a与非a的判断的基础上，a之后再b非b，非a后面在c非c，这样不断下去，
并且后续的判断会在前次的基础上更深层次，有一定的关联。
逻辑回归就是神经网络的每个神经元。总结一句话：逻辑回归是单神经元，神经网络是多神经元、层叠组合的复杂结构。

> 直观类比
>- 逻辑回归：老师只看一眼作业，说“对/不对”。
>- 神经网络：老师先看你字迹（第一层），再看你公式（第二层），再看你思路（第三层），最后才下结论。

神经网络在逻辑回归的基础上，通过多层神经元（节点）的组合，构建更复杂的决策过程：

- 多层结构：输入经过多层处理，每层包含多个神经元，每一层提取更高层次的特征。
- 非线性激活：每层神经元不仅做线性组合 $\mathbf{w}^T\mathbf{x} + b$，还通过非线性激活函数（如 ReLU、sigmoid、tanh）引入非线性，使模型能处理复杂模式。
$  h^{(l)} = f(\mathbf{W}^{(l)}h^{(l-1)} + \mathbf{b}^{(l)})  $
其中，$h^{(l)}$ 是第 $l$ 层的输出，$f$ 是激活函数，$h^{(l-1)}$ 是前一层的输入。
- 层间关联：每一层的输出是下一层的输入，层层递进，逐步提取更抽象的特征。

直观理解：逻辑回归只看“表面”（一次线性判断），而神经网络像“层层剖析”，从低级特征（字迹）到高级特征（思路），逐步做出综合判断。

神经网络模型建立在很多神经元之上，每一个神经元又是一个个学习模型。
这些 **神经元（也叫激活单元，activation unit** 采纳一些特征作为输出，并且根据本身的模型提供一个输出。
在神经网络中，参数又可被称为权重（weight）。

> 神经网络的核心思想是：通过多层“神经元”（逻辑单元）把输入映射到输出，每一层都做非线性变换。
数学上，一个神经网络可以表示为：
$$
\mathbf{a}^{[l]} = f( \mathbf{W}^{[l]} \mathbf{a}^{[l-1]} + \mathbf{b}^{[l]} )
$$
>- $\mathbf{a}^{[l]}$：第$l$层的输出（激活值）
>- $\mathbf{W}^{[l]}$：第$l$层权重矩阵
>- $\mathbf{b}^{[l]}$：第$l$层偏置向量——
> 偏置向量（bias vector）是神经网络中每一层神经元的额外可学习参数，它不依赖输入，用来调节激活函数的输出。
> 相当于在逻辑回归里判定边界的平移量
>- $f$：非线性激活函数（如sigmoid, ReLU等）
>- 第0层$\mathbf{a}^{[0]}$就是输入特征$x$

---

## 神经网络前向传播

<div style="display: flex; justify-content: center; gap: 20px;">
&nbsp;&nbsp;<img src="./images/Three-Layer%20Neural%20Network%20with%20Bias%20Units.png" alt="图示" style="width: 90%; max-width: 600px; min-width: 300px;">
</div>

### 1. 神经网络结构

考虑一个三层神经网络：

- **输入层**：4 个节点（$x_0, x_1, x_2, x_3$，其中 $x_0=1$ 为偏置项）。
- **隐藏层**：3 个神经元（$a_1^{(2)}, a_2^{(2)}, a_3^{(2)}$，输出包含偏置项 $a_0^{(2)}=1$）。
- **输出层**：1 个神经元（$h_\theta(x)$，例如二分类问题的概率输出）。

### 2. 符号定义

- $a_i^{(j)}$：第 $j$ 层第 $i$ 个神经元的激活值，$a_0^{(j)}=1$ 为偏置单元。
- $\theta^{(j)}$：从第 $j$ 层到第 $j+1$ 层的权重矩阵，维度为 (第 $j+1$ 层神经元数) $\times$ (第 $j$ 层神经元数 + 1)。
- $z^{(j)}$：第 $j$ 层的线性输入，$z^{(j)} = \theta^{(j-1)} a^{(j-1)}$（单样本，含偏置）。
- $X$：输入数据，单样本为 $X \in \mathbb{R}^{(n+1) \times 1}$（含偏置），训练集为 $X \in \mathbb{R}^{m \times (n+1)}$（$m$ 个样本，$n+1$ 个特征）。
- $g(\cdot)$：激活函数（如 sigmoid、ReLU、softmax，输出层根据任务选择）。

### 3. 单样本前向传播

#### (1) 隐藏层计算

对于单样本输入 $X = [x_0, x_1, x_2, x_3]^T \in \mathbb{R}^{4 \times 1}$（$x_0=1$ 为偏置项），隐藏层有 3 个神经元，计算如下：
$$
z_1^{(2)} = \theta_{10}^{(1)} x_0 + \theta_{11}^{(1)} x_1 + \theta_{12}^{(1)} x_2 + \theta_{13}^{(1)} x_3, \quad a_1^{(2)} = g(z_1^{(2)})
$$

$$
z_2^{(2)} = \theta_{20}^{(1)} x_0 + \theta_{21}^{(1)} x_1 + \theta_{22}^{(1)} x_2 + \theta_{23}^{(1)} x_3, \quad a_2^{(2)} = g(z_2^{(2)})
$$

$$
z_3^{(2)} = \theta_{30}^{(1)} x_0 + \theta_{31}^{(1)} x_1 + \theta_{32}^{(1)} x_2 + \theta_{33}^{(1)} x_3, \quad a_3^{(2)} = g(z_3^{(2)})
$$

**矩阵形式**：
权重矩阵 $\theta^{(1)} \in \mathbb{R}^{3 \times 4}$：
$$
\theta^{(1)} = \begin{bmatrix}
\theta_{10}^{(1)} & \theta_{11}^{(1)} & \theta_{12}^{(1)} & \theta_{13}^{(1)} \\
\theta_{20}^{(1)} & \theta_{21}^{(1)} & \theta_{22}^{(1)} & \theta_{23}^{(1)} \\
\theta_{30}^{(1)} & \theta_{31}^{(1)} & \theta_{32}^{(1)} & \theta_{33}^{(1)}
\end{bmatrix}
$$
线性输入：
$$
z^{(2)} = \theta^{(1)} X \in \mathbb{R}^{3 \times 1}, \quad a^{(2)} = g(z^{(2)}) = [a_1^{(2)}, a_2^{(2)}, a_3^{(2)}]^T
$$
其中，$g$ 逐元素应用（例如，$g(z) = \frac{1}{1 + e^{-z}}$ 为 sigmoid）。

#### (2) 输出层计算

隐藏层激活（含偏置）：$a^{(2)} = [a_0^{(2)}, a_1^{(2)}, a_2^{(2)}, a_3^{(2)}]^T \in \mathbb{R}^{4 \times 1}$，$a_0^{(2)}=1$。
权重矩阵：$\theta^{(2)} = [\theta_{10}^{(2)}, \theta_{11}^{(2)}, \theta_{12}^{(2)}, \theta_{13}^{(2)}] \in \mathbb{R}^{1 \times 4}$。
输出：
$$
z^{(3)} = \theta_{10}^{(2)} a_0^{(2)} + \theta_{11}^{(2)} a_1^{(2)} + \theta_{12}^{(2)} a_2^{(2)} + \theta_{13}^{(2)} a_3^{(2)}
$$

$$
h_\theta(x) = g(z^{(3)})
$$

**矩阵形式**：
$$
z^{(3)} = \theta^{(2)} a^{(2)} \in \mathbb{R}, \quad h_\theta(x) = g(z^{(3)})
$$
对于二分类，$g$ 通常为 sigmoid；对于多分类，$g$ 为 softmax。

### 4. 训练集向量化

对于训练集（$m$ 个样本），输入矩阵 $X \in \mathbb{R}^{m \times 4}$，每行一个样本（含偏置 $x_0^{(i)}=1$）：
$$
X = \begin{bmatrix}
x_0^{(1)} & x_1^{(1)} & x_2^{(1)} & x_3^{(1)} \\
x_0^{(2)} & x_1^{(2)} & x_2^{(2)} & x_3^{(2)} \\
\vdots & \vdots & \vdots & \vdots \\
x_0^{(m)} & x_1^{(m)} & x_2^{(m)} & x_3^{(m)}
\end{bmatrix}
$$

**隐藏层**：
$$
Z^{(2)} = X \theta^{(1)T} \in \mathbb{R}^{m \times 3}, \quad A^{(2)} = g(Z^{(2)})
$$
其中，$\theta^{(1)T} \in \mathbb{R}^{4 \times 3}$，$A^{(2)} \in \mathbb{R}^{m \times 3}$ 是 $m$ 个样本的隐藏层激活。

**输出层**：
为每个样本添加隐藏层偏置项，构造 $A^{(2)} = [1, A^{(2)}] \in \mathbb{R}^{m \times 4}$（在每行前添加 1），然后：
$$
Z^{(3)} = A^{(2)} \theta^{(2)T} \in \mathbb{R}^{m \times 1}, \quad H_\theta(X) = g(Z^{(3)})
$$

向量化计算避免了循环，提高了效率。

### 5. 前向传播总结

前向传播的步骤如下：

1. **输入层**：$A^{(1)} = X$，单样本 $X \in \mathbb{R}^{4 \times 1}$（含偏置），多样本 $X \in \mathbb{R}^{m \times 4}$。
2. **隐藏层**：
   - 单样本：$z^{(2)} = \theta^{(1)} A^{(1)}$，$a^{(2)} = g(z^{(2)})$。
   - 多样本：$Z^{(2)} = X \theta^{(1)T}$，$A^{(2)} = g(Z^{(2)})$。
3. **输出层**：
   - 单样本：$z^{(3)} = \theta^{(2)} a^{(2)}$，$h_\theta(x) = g(z^{(3)})$。
   - 多样本：$Z^{(3)} = A^{(2)} \theta^{(2)T}$，$H_\theta(X) = g(Z^{(3)})$。

**注**：

- 偏置项通过 $x_0=1$ 和 $a_0^{(2)}=1$ 融入权重矩阵计算，简化公式。
- 激活函数 $g$ 根据任务选择，例如 sigmoid 用于二分类，softmax 用于多分类。

---

## 神经网络应用

神经网络中，单层神经元（无中间层）的计算可用来表示逻辑运算，比如逻辑与(AND)、逻辑或(OR)。

将一个逻辑单元（神经元）这样表示：
$$
h_{\theta}(x) = g( \theta_0 + \theta_1 x_1 + \theta_2 x_2 )
$$

这里：

- $g(z)$ 是激活函数，常见选择是 sigmoid： $g(z) = \frac{1}{1+e^{-z}}$。

- $x_1, x_2$ 是输入（0 或 1）。

- $\theta$ 是参数（权重+偏置），通过学习来决定。

### _逻辑与（AND）_
真值表

|$x_1$|$x_2$| $h_{\theta}(x)$  |
|---|---|------------------|
|0|0|0|
|0|1|0|
|1|1|1|
|1|0|0|

> 如果我们要让神经元$(h_{\theta}(x) = g( \theta_0 + \theta_1 x_1 + \theta_2 x_2 )）$实现 AND 运算， $\theta_0, \theta_1, \theta_2$ 应该满足什么特点？
> 
> 只有当 $x_1=1, x_2=1$ 时，输出接近 1，其他情况输出接近 0。
> 先假设 $ \theta_0 = 0，\theta_1 = 0.5，\theta_2 = 0.5 $
> - 当 $(x_1, x_2)=(1,1)$： （还不错，接近 1）
> - 当 $(x_1, x_2)=(1,0)$ 或 $(0,1)$： （这离 0 有点远）
> - 当 $(x_1, x_2)=(0,0)$： （完全没达到想要的 0）
> 
> 👉 所以问题在哪？——主要是 偏置太小。sigmoid 在 0 的时候会输出 0.5，而不是接近 0。
> 要实现 AND，通常要 把阈值设高一点：让单个输入不足以超过阈值，必须两个输入一起才行。
>
> 比如：
>  $ \theta_0 = -0.8，\theta_1 = 1，\theta_2 = 1 $
> 
> 这样：
> <div style="display: flex; justify-content: center; gap: 20px;">
> &nbsp;&nbsp;<img src="./images/AND.png" alt="图示" style="width: 90%; max-width: 600px; min-width: 300px;">
> </div>
>
> - (1,1): $z= -0.8 + 1+1 = 1.2 \to g(1.2)\approx 0.77 \approx 1$
> - (1,0): $z= -0.8 + 1 = 0.2 \to g(0.2)\approx 0.55$ （更接近 0）
> - (0,0): $z=-0.8 \to g(-0.8)\approx 0.31 \approx 0$

### _逻辑或(or)_
真值表

|$x_1$|$x_2$| $h_{\theta}(x)$ |
|---|---|-----------------|
|0|0| 0               |
|0|1| 1               |
|1|1| 1               |
|1|0| 1               |

> 如果我们要让神经元$(h_{\theta}(x) = g( \theta_0 + \theta_1 x_1 + \theta_2 x_2 )）$实现 OR 运算， $\theta_0, \theta_1, \theta_2$ 应该满足什么特点？
> 
> 比如：
>  $ \theta_0 = -0.8，\theta_1 = 1，\theta_2 = 1 $
> 
>这样：
> <div style="display: flex; justify-content: center; gap: 20px;">
> &nbsp;&nbsp;<img src="./images/OR.png" alt="图示" style="width: 90%; max-width: 600px; min-width: 300px;">
> </div>
>
> - (1,0): $z = 0.5 \to g(0.5)\approx 0.62$ 
> - (0,1): 同样 $0.62$
> - (1,1): $z = 1.5 \to g(1.5)\approx 0.82$
> - (0,0): $z=-0.5 \to g(-0.5)\approx 0.38$

用单个神经元可以做 AND / OR，那 XOR（异或）/ XNOR(同或) 能不能用单个神经元表示？

单个神经元的形式是：$ h_{\theta}(x) = g( \theta_0 + \theta_1 x_1 + \theta_2 x_2 ) $,
这个是一个线性分类器，本质上是“画一条直线”把 0 和 1 分开。
但是 XOR 的分布是什么样的呢？
- 正例 (1)：在 (0,1) 和 (1,0)
- 负例 (0)：在 (0,0) 和 (1,1)
画一下就能发现：这两个正例在对角线，负例在另一条对角线。

👉 它们不是用一条直线能分开的，而是线性不可分。

所以，不管你怎么调 $\theta_0,\theta_1,\theta_2$，单个神经元是做不到 XOR 的（XNOR同理）。

### _异或(XOR)_
真值表

|$x_1$|$x_2$| $h_{\theta}(x)$ |
|---|---|-----------------|
|0|0| 0               |
|0|1| 1               |
|1|1| 0               |
|1|0| 1               |
> 如果我们要让神经元实现 XOR 运算，需要两层，输入层到隐藏层两个神经元，隐藏层到输出层需要一个神经元，涉及到 $\theta$ 应该满足什么特点？
> 
> 比如：
> - 输入层到隐藏层y1 $ \theta_0 = 0.5，\theta_1 = 1.0，\theta_2 = 1.0 $
> - 输入层到隐藏层y2 $ \theta_0 = 1.0，\theta_1 = 1.5，\theta_2 = 1.0 $
> - 隐藏层到输出层h(x)：$ \theta_0 = 2.0，\theta_1 = 0.5，\theta_2 = -1.0 $
> 
> 这样：
> <div style="display: flex; justify-content: center; gap: 20px;">
> &nbsp;&nbsp;<img src="./images/XOR.png" alt="图示" style="width: 90%; max-width: 600px; min-width: 300px;">
> </div>

### _同或(XNOR)_
真值表

|$x_1$|$x_2$| $h_{\theta}(x)$ |
|---|---|-----------------|
|0|0| 1               |
|0|1| 0               |
|1|1| 1               |
|1|0| 0               |

> 如果我们要让神经元实现 XNOR 运算， 涉及到 $\theta$ 应该满足什么特点？
> 
> 比如：
> - 输入层到隐藏层y1：$ \theta_0 = 0.5，\theta_1 = 1.0，\theta_2 = 1.0 $
> - 输入层到隐藏层y2：$ \theta_0 = 1.0，\theta_1 = -1.5，\theta_2 = 1.0 $
> - 隐藏层到输出层h(x)：$ \theta_0 = 2.0，\theta_1 = 0.5，\theta_2 = -1.0 $
> 这样：
> <div style="display: flex; justify-content: center; gap: 20px;">
> &nbsp;&nbsp;<img src="./images/XNOR.png" alt="图示" style="width: 90%; max-width: 600px; min-width: 300px;">
> </div>