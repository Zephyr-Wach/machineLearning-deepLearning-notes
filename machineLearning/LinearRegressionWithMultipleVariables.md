# å¤šå˜é‡çº¿æ€§å›å½’ (Linear Regression with Multiple Variables)

å¤šå˜é‡çº¿æ€§å›å½’ç”¨äºé¢„æµ‹ä¸€ä¸ªè¿ç»­å˜é‡ï¼ˆç›®æ ‡ï¼‰ä¸å¤šä¸ªç‰¹å¾ï¼ˆè‡ªå˜é‡ï¼‰ä¹‹é—´çš„çº¿æ€§å…³ç³»ã€‚å®ƒæ˜¯æœºå™¨å­¦ä¹ ä¸­åŸºç¡€çš„å›å½’æ–¹æ³•ä¹‹ä¸€ï¼Œèƒ½å¤Ÿæ•æ‰å¤šä¸ªå› ç´ å¯¹ç›®æ ‡å˜é‡çš„ç»¼åˆå½±å“ã€‚

[ä¸Šä¸€ç« ](./LinearRegressionWithOneVariable.md) --- [ä¸‹ä¸€ç« ]()

---

è¿™éƒ¨åˆ†ç»“åˆä¸€ä¸ªæˆ¿ä»·éšæˆ¿å­å¤§å°å’Œæˆ¿å­è·ç¦»å­¦æ ¡è·ç¦»è¿œè¿‘çš„[æ•°æ®é›†](./dateSet/priceBymultiLinear-dataset.csv)æ¥ç†è§£ï¼Œ
æ•°æ®éƒ½æ˜¯ç”±grokç”Ÿæˆï¼Œç±»ä¼¼è¿™æ ·;å’Œå•å˜é‡çº¿æ€§å›å½’ä¸­ç›¸ä¼¼çš„å®šä¹‰ä¸å†åšé€šä¿—è§£é‡Šã€‚

|æˆ¿å­å¤§å° (å¹³æ–¹ç±³)|æ¥¼å±‚|å¹´é™ (å¹´)|è·å­¦æ ¡è·ç¦» (å…¬é‡Œ)|æˆ¿ä»· (å…ƒ)|
|-----------   |---|--------|-------|-------|
|106.18|7|26.5|1.5|677784.0|
|192.61|17|12.1|3.0|1224394.0|
|159.8|20|4.7|0.25|1087393.0|
|139.8|4|44.9|0.28|753724.0|
|73.4|5|45.0|4.13|470202.0|
|73.4|7|31.7|1.86|434783.0|

---

## 1.å¤šç»´ç‰¹å¾

åœ¨ä¹‹å‰çš„æˆ¿ä»·é¢„æµ‹é—®é¢˜é‡Œï¼Œåªè€ƒè™‘åˆ°æˆ¿å±‹å°ºå¯¸ä¸€ä¸ªç‰¹å¾ï¼Œè¿™é‡Œæˆ‘ä»¬è€ƒè™‘å¤šä¸ªç‰¹å¾çš„é—®é¢˜ã€‚
æ¯”å¦‚åœ¨æˆ¿ä»·é¢„æµ‹é—®é¢˜ä¸­ï¼Œå¼•å…¥æˆ¿é—´æ•°ã€æ¥¼å±‚ã€å¹´é™ç­‰ã€‚
æ­¤æ—¶çš„æ¨¡å‹ï¼š
$$h_\theta(x) = \theta_0 + \theta_1 x_1 + \theta_2 x_2 + \dots + \theta_n x_n$$
ä¸Šè¿°å…¬å¼ä¸­æœ‰ n+1 ä¸ªå‚æ•°å’Œ n ä¸ªå˜é‡ï¼Œä¸ºäº†ç®€åŒ–å…¬å¼ï¼Œå¼•å…¥ x0 = 1ï¼Œåˆ™ä¸Šå¼å†™ä½œï¼š
$$h_\theta(x) = \theta_0 x_0 + \theta_1 x_1 + \theta_2 x_2 + \dots + \theta_n x_n$$
å¦‚æ­¤æ”¹å†™å‘é‡å½¢å¼(å…¶ä¸­ï¼ŒT ä»£è¡¨çŸ©é˜µè½¬ç½®)ï¼š
$$h_\theta(x) = \theta^T X$$

ç„¶ååœ¨è¿™æ ·ä¸€ä¸ªå¤šå˜é‡çº¿æ€§å›å½’ä¸­ï¼Œæˆ‘ä»¬ä¹Ÿæ„å»ºä¸€ä¸ªä»£ä»·å‡½æ•°ï¼Œåˆ™è¿™ä¸ªä»£ä»·å‡½æ•°æ˜¯æ‰€æœ‰å»ºæ¨¡è¯¯å·®çš„å¹³æ–¹å’Œï¼Œ
ç”¨æ¥è¡¡é‡æ¨¡å‹é¢„æµ‹å€¼ä¸çœŸå®å€¼ä¹‹é—´çš„è¯¯å·®ï¼Œå®šä¹‰ä¸ºï¼š

$$
J(\theta_0,\theta_1...\theta_n) = \frac{1}{2m} \sum_{i=1}^{m} \Big( h_\theta(x^{(i)}) - y^{(i)} \Big)^2
$$

å…¶ä¸­ï¼š
- $ m $ ä¸ºè®­ç»ƒæ ·æœ¬æ•°é‡
- $ h_\theta(x^{(i)}) $ ä¸ºç¬¬ $ i $ ä¸ªæ ·æœ¬çš„é¢„æµ‹å€¼
- $ y^{(i)} $ ä¸ºç¬¬ $ i $ ä¸ªæ ·æœ¬çš„çœŸå®å€¼

ä¹Ÿå¯ä»¥å‡†å¤‡ä¸€ä¸ªpythonä»£ç æ¥è®¡ç®—ä»£ä»·å‡½æ•°ï¼š
```python
def compute_cost(X, y, theta):
    inner = np.power(((X * theta.T) - y), 2)
    return np.sum(inner) / (2 * len(X))
```

## 2.å¤šå˜é‡æ¢¯åº¦ä¸‹é™
å¤šå˜é‡æ¢¯åº¦ä¸‹é™çš„ç›®æ ‡å’Œå•å˜é‡çº¿æ€§å›å½’é—®é¢˜ä¸­ä¸€æ ·ï¼Œè¦æ‰¾å‡ºä½¿å¾—ä»£ä»·å‡½æ•°æœ€å°çš„ä¸€ç³»åˆ—å‚æ•°ã€‚

æ¢¯åº¦ä¸‹é™çš„æ ¸å¿ƒæ€æƒ³æ˜¯ï¼š  
- **æ¢¯åº¦ï¼ˆgradientï¼‰**ï¼š$\nabla J(\theta)$ æŒ‡å‘å‡½æ•°å¢é•¿æœ€å¿«çš„æ–¹å‘  
- **æœ€å°åŒ–ç›®æ ‡**ï¼šæˆ‘ä»¬è¦å¾€åæ–¹å‘èµ°ï¼Œé€æ­¥é™ä½ $J(\theta)$  

å› æ­¤æ¯ä¸€æ­¥æ›´æ–°å…¬å¼ä¸ºï¼š

$$
\theta_j := \theta_j - \alpha \cdot \frac{\partial}{\partial \theta_j} J(\theta_0,\theta_1...\theta_n)
$$

è¿™é‡Œï¼š
- $\alpha$ æ˜¯å­¦ä¹ ç‡ (learning rate)ï¼Œå†³å®šæ­¥é•¿  
- $\frac{\partial}{\partial \theta_j} J(\theta)$ æ˜¯ä»£ä»·å‡½æ•°å¯¹å‚æ•° $\theta_j$ çš„åå¯¼ï¼Œè¡¨ç¤º $J(\theta)$ å¯¹è¯¥å‚æ•°çš„æ•æ„Ÿç¨‹åº¦  

å¯¹ $\theta_j$ æ±‚åå¯¼ï¼š

$$
\frac{\partial}{\partial \theta_j} J(\theta) 
= \frac{1}{2m} \cdot 2 \sum_{i=1}^{m} \Big( \theta^T x^{(i)} - y^{(i)} \Big) \cdot x_j^{(i)}
$$

åŒ–ç®€å¾—ï¼š

$$
\frac{\partial}{\partial \theta_j} J(\theta) 
= \frac{1}{m} \sum_{i=1}^{m} \Big( h_\theta(x^{(i)}) - y^{(i)} \Big) x_j^{(i)}
$$

ä»£å…¥æ¢¯åº¦ä¸‹é™æ›´æ–°è§„åˆ™ï¼š

$$
\theta_j := \theta_j - \alpha \cdot \frac{1}{m} \sum_{i=1}^{m} \Big( h_\theta(x^{(i)}) - y^{(i)} \Big) x_j^{(i)}, 
\quad j=0,1,\dots,n
$$

è¿™æ ·æˆ‘å¾—åˆ°äº†ä¸¤ä¸ªå…¬å¼ï¼š

1. æ ¸å¿ƒå…¬å¼ï¼ˆé€šç”¨å½¢å¼ï¼‰

> $$
\theta_j := \theta_j - \alpha \cdot \frac{\partial}{\partial \theta_j} J(\theta_0,\theta_1...\theta_n)
$$

ç‰¹ç‚¹ï¼šæŠ½è±¡ã€é€šç”¨

é€‚ç”¨åœºæ™¯ï¼š

- ä»»ä½•æœºå™¨å­¦ä¹ æ¨¡å‹çš„æ¢¯åº¦ä¸‹é™æ›´æ–°éƒ½å¯ä»¥ç”¨è¿™ä¸ªå…¬å¼

- ä¸ä¾èµ–å…·ä½“çš„ä»£ä»·å‡½æ•°ï¼Œåªè¦èƒ½ç®—å‡ºåå¯¼å°±èƒ½ç”¨

ä¼˜ç‚¹ï¼šæ¦‚å¿µæ¸…æ™°ï¼Œæ–¹ä¾¿æ¨å¯¼ä¸åŒæ¨¡å‹çš„æ¢¯åº¦

ä¾‹å­ï¼šçº¿æ€§å›å½’ã€é€»è¾‘å›å½’ã€ç¥ç»ç½‘ç»œç­‰ä»»ä½•æ¢¯åº¦ä¸‹é™éƒ½ç”¨è¿™ä¸ªå…¬å¼å¼€å§‹

2. æœ€ç»ˆå…¬å¼ï¼ˆçº¿æ€§å›å½’ä¸“ç”¨ï¼‰

> $$
\theta_j := \theta_j - \alpha \cdot \frac{1}{m} \sum_{i=1}^{m} \Big( h_\theta(x^{(i)}) - y^{(i)} \Big) x_j^{(i)}, 
\quad j=0,1,\dots,n
$$

ç‰¹ç‚¹ï¼šå…·ä½“ã€çº¿æ€§å›å½’ä¸“ç”¨

é€‚ç”¨åœºæ™¯ï¼š

- å·²çŸ¥ä»£ä»·å‡½æ•°æ˜¯çº¿æ€§å›å½’çš„å‡æ–¹è¯¯å·®

- ç›´æ¥åœ¨ä»£ç é‡Œå®ç°æ¢¯åº¦æ›´æ–°

ä¼˜ç‚¹ï¼šä¸ç”¨å†æ±‚åå¯¼ï¼Œç›´æ¥å†™ä»£ç å³å¯


_ï¼Ÿ é‚£ä¹ˆæ–°çš„é—®é¢˜ï¼Œä¸ºä»€ä¹ˆè¦æ±‚åå¯¼ï¼Œé€šç”¨å…¬å¼ä¸å¥½å— ï¼Ÿ_

æ ¸å¿ƒå…¬å¼æœ¬èº«æ˜¯æŠ½è±¡çš„é€šç”¨å½¢å¼ï¼Œå®ƒæœ¬èº«æ— æ³•ç›´æ¥è®¡ç®—æ•°å€¼æ›´æ–°ï¼Œæ‰€ä»¥å¿…é¡»æ±‚åå¯¼æ‰èƒ½ç”¨åœ¨å…·ä½“æ¨¡å‹é‡Œã€‚
å› æ­¤å®é™…è®­ç»ƒæ¨¡å‹æ—¶éƒ½æ˜¯ç”¨æœ€ç»ˆå…¬å¼ï¼Œè€Œä¸”å¯¹çº¿æ€§å›å½’ã€é€»è¾‘å›å½’ç­‰å¸¸ç”¨æ¨¡å‹ï¼Œæœ€ç»ˆå…¬å¼ç›´æ¥å‘é‡åŒ–åè®¡ç®—éå¸¸å¿«ï¼Œ
æ ¸å¿ƒå…¬å¼ä¸èƒ½ç›´æ¥å‘é‡åŒ–ï¼Œéœ€è¦å…ˆæ±‚åå¯¼

---

## 3.ç‰¹å¾å’Œå¤šé¡¹å¼å›å½’

### 1.ç‰¹å¾ï¼ˆFeatureï¼‰

- **æ¦‚å¿µ**ï¼šæ¨¡å‹ç”¨æ¥é¢„æµ‹ç›®æ ‡ \(y\) çš„è¾“å…¥ä¿¡æ¯ã€‚  
- **äº§ç”ŸèƒŒæ™¯**ï¼š
  - æ•°æ®å¤æ‚ï¼Œä¸€ä¸ªç»“æœå¯èƒ½å—å¤šä¸ªå› ç´ å½±å“ã€‚
  - ä¸ºäº†è®©æ¨¡å‹â€œçœ‹æ‡‚â€è¿™äº›å› ç´ ï¼ŒæŠŠå®ƒä»¬æŠ½è±¡ä¸ºç‰¹å¾ã€‚
- **ä¾‹å­**ï¼š
  - æˆ¿ä»·é¢„æµ‹ï¼šé¢ç§¯ã€æ¥¼å±‚ã€ä½ç½® â†’ éƒ½æ˜¯ç‰¹å¾  
  - å­¦ç”Ÿæˆç»©é¢„æµ‹ï¼šä¸Šè¯¾æ—¶é—´ã€ä½œä¸šé‡ã€ç¡çœ æ—¶é—´ â†’ ç‰¹å¾  

**å¤šç‰¹å¾çº¿æ€§å›å½’å…¬å¼**ï¼š

$$
h_\theta(x) = \theta_0 + \theta_1 x_1 + \theta_2 x_2 + \dots + \theta_n x_n
$$

- ç‰¹å¾è¶Šå¤šï¼Œæ¨¡å‹ä¿¡æ¯è¶Šä¸°å¯Œï¼Œé¢„æµ‹èƒ½åŠ›å¯èƒ½è¶Šå¼º  
- ä½†è¿‡å¤šæˆ–ä¸ç›¸å…³ç‰¹å¾ä¼šé€ æˆ**è¿‡æ‹Ÿåˆ**
> è¿‡æ‹Ÿåˆï¼šæ¨¡å‹å¤ªå¤æ‚ï¼Œå­¦åˆ°äº†è®­ç»ƒæ•°æ®é‡Œçš„å™ªå£°ï¼Œè€Œä¸æ˜¯æ•°æ®çš„çœŸå®è§„å¾‹ã€‚
ä»è€Œå¯¼è‡´æ¨¡å‹åœ¨è®­ç»ƒæ•°æ®ä¸Šè¡¨ç°å¾ˆå¥½ï¼Œä½†åœ¨æ–°æ•°æ®ï¼ˆæµ‹è¯•é›†ï¼‰ä¸Šè¡¨ç°å·®ã€‚
> é€šä¿—ç†è§£ï¼šè¿‡æ‹Ÿåˆå°±åƒè€ƒè¯•æ—¶åªè®°ä½äº†è€å¸ˆè®²çš„ä¾‹é¢˜ï¼Œè€Œä¸ä¼šç†è§£æ–¹æ³•ï¼Œé‡åˆ°æ–°é¢˜å°±ä¸ä¼šåšã€‚

> ç‰¹å¾ç¼©æ”¾ï¼ˆFeature Scalingï¼‰
> æ¦‚å¿µï¼šæŠŠç‰¹å¾çš„æ•°å€¼å°ºåº¦ç»Ÿä¸€åˆ°ç›¸è¿‘èŒƒå›´ï¼Œå¸¸ç”¨æ–¹æ³•ï¼š
> - å½’ä¸€åŒ–ï¼ˆMin-Max Scalingï¼‰ï¼šæŠŠæ•°æ®ç¼©æ”¾åˆ° [0,1]
> - æ ‡å‡†åŒ–ï¼ˆStandardizationï¼‰ï¼šä½¿æ•°æ®å‡å€¼ä¸º 0ï¼Œæ–¹å·®ä¸º 1
> 
> ä¸ºä»€ä¹ˆè¦åšï¼š 
> æ¢¯åº¦ä¸‹é™æ”¶æ•›æ›´å¿«,é¿å…æŸäº›å¤§å°ºåº¦ç‰¹å¾ä¸»å¯¼æ¢¯åº¦æ›´æ–° 
> 
> ä¾‹å­ï¼š 
> é¢ç§¯åœ¨ 100200 å¹³æ–¹ç±³ï¼Œä»·æ ¼åœ¨ 10100 ä¸‡ ,
> ä¸ç¼©æ”¾æ—¶ï¼Œæ¢¯åº¦ä¸‹é™æ›´æ–°ä¸»è¦å—ä»·æ ¼å½±å“ï¼Œæ¨¡å‹éš¾è®­ç»ƒ

---

### 2ï¸.å¤šé¡¹å¼å›å½’ï¼ˆPolynomial Regressionï¼‰

- **æ¦‚å¿µ**ï¼šä¸ºæ‹Ÿåˆéçº¿æ€§æ•°æ®ï¼ŒæŠŠåŸç‰¹å¾è¿›è¡Œå‡é˜¶æ‰©å±•ï¼Œç„¶åç”¨çº¿æ€§å›å½’æ‹Ÿåˆã€‚  
- **äº§ç”ŸèƒŒæ™¯**ï¼š
  - æ•°æ®å‘ˆæ›²çº¿å…³ç³»ï¼Œç›´çº¿æ— æ³•æ‹Ÿåˆ  
  - å¸Œæœ›ä¿ç•™çº¿æ€§å›å½’æ±‚è§£ç®€å•ã€æ¢¯åº¦ä¸‹é™å¯ç”¨çš„ä¼˜ç‚¹  
- **æ–¹æ³•**ï¼š
  - å¯¹åŸç‰¹å¾ \(x\) æ‰©å±•æˆ \(x^2, x^3, â€¦, x^d\)  
  - å°†è¿™äº›æ–°ç‰¹å¾åŠ å…¥æ¨¡å‹ï¼Œå½¢æˆå¤šé¡¹å¼ç‰¹å¾å‘é‡ \(x_{\text{poly}}\)  

**å¤šé¡¹å¼å›å½’å…¬å¼**ï¼š

$$
h_\theta(x) = \theta_0 + \theta_1 x + \theta_2 x^2 + \dots + \theta_d x^d
$$

- å‚æ•° $\theta_j$ å¯¹åº”æ¯ä¸ªç‰¹å¾çº¿æ€§  
- å›¾åƒå¯ä»¥æ˜¯æ›²çº¿ï¼Œä½†æ¨¡å‹ä»æ˜¯çº¿æ€§å›å½’ï¼ˆå‚æ•°çº¿æ€§ï¼‰

---

### 3.ç‰¹å¾æ‰©å±•ç¤ºæ„(è§£é‡Š2ä¸­å¦‚ä½•æ‹Ÿåˆéçº¿æ€§æ•°æ®)

### 1ï¸.åŸå§‹ç‰¹å¾
- å‡è®¾åªæœ‰ä¸€ä¸ªç‰¹å¾ $x$
- æ•°æ®å‘é‡å½¢å¼ï¼š
$$
x = [x_1, x_2, ..., x_m]
$$
æ¯ä¸ª$x_i$æ˜¯ä¸€ä¸ªæ ·æœ¬çš„ç‰¹å¾å€¼

### 2ï¸.ä¸ºä»€ä¹ˆè¦æ‰©å±•
- æ•°æ®å‘ˆéçº¿æ€§å…³ç³»æ—¶ï¼Œå•ä¸ªç‰¹å¾æ— æ³•æ‹Ÿåˆæ›²çº¿  
- å¸Œæœ›çº¿æ€§æ¨¡å‹ä¹Ÿèƒ½æ‹Ÿåˆæ›²çº¿ â†’ å°±æŠŠç‰¹å¾å‡é˜¶ï¼Œå½¢æˆå¤šé¡¹å¼ç‰¹å¾

### 3ï¸.æ‰©å±•åç‰¹å¾
- ä¾‹å¦‚ 2 é˜¶å¤šé¡¹å¼ï¼š
$$
x_{\text{poly}} = [1, x, x^2]
$$
  - 1 â†’ æˆªè·é¡¹  
  - x â†’ åŸå§‹ç‰¹å¾  
  - xÂ² â†’ å‡é˜¶ç‰¹å¾  

- æ¯ä¸ªæ ·æœ¬ä» 1 ä¸ªç‰¹å¾å˜æˆ 3 ä¸ªç‰¹å¾

### 4ï¸.ä½œç”¨
- æ¨¡å‹ä»æ˜¯çº¿æ€§å›å½’ï¼ˆå‚æ•°çº¿æ€§ï¼‰  
- æœ‰ $x^2$ è¿™ä¸ªç‰¹å¾åï¼Œæ‹Ÿåˆå‡ºçš„æ›²çº¿å¯ä»¥å¼¯æ›²  
- è®­ç»ƒæ—¶æ¢¯åº¦ä¸‹é™ä¼šä¸ºæ¯ä¸ªç‰¹å¾å­¦ä¹ ä¸€ä¸ªæƒé‡ $\theta_j$

### ğŸ”¹ é€šä¿—æ¯”å–»
- åŸå§‹ç‰¹å¾ $x$ â†’ åªæœ‰ä¸€æ¡çº¿ç´¢  
- æ‰©å±•ç‰¹å¾ $x, x^2, x^3$ â†’ ç»™æ¨¡å‹æ›´å¤šâ€œè§†è§’â€ï¼Œå¯ä»¥ç†è§£æ›²çº¿è¶‹åŠ¿  
- æ¨¡å‹ç”¨æ¯æ¡çº¿ç´¢çš„æƒé‡ $\theta_j$ æ¥ç»¼åˆåˆ¤æ–­ï¼Œæ‹Ÿåˆå‡ºæ›²çº¿  

### ğŸ”¹ æ€»ç»“
> â€œç‰¹å¾æ‰©å±•ç¤ºæ„â€å°±æ˜¯å±•ç¤ºå¦‚ä½•æŠŠåŸå§‹ç‰¹å¾é€šè¿‡å‡é˜¶æˆ–å˜æ¢ï¼Œå˜æˆæ›´å¤šå¯ç”¨ç‰¹å¾ï¼Œè®©çº¿æ€§æ¨¡å‹å¯ä»¥æ‹Ÿåˆéçº¿æ€§æ•°æ®ã€‚

> è¿™é‡Œæˆ‘çœ‹åˆ°æ—¶æœ‰ç–‘é—®ï¼šå¤šé¡¹å¼å›å½’ä¸­ï¼Œä¸ºä»€ä¹ˆé‡‡ç”¨å¹³å‡¡ã€ç«‹æ–¹è¿™äº›æ–¹æ³•æ¥æ‹Ÿåˆéçº¿æ€§æ•°æ®ï¼Œè¿™æ ·ä¸ä¼šå¯¼è‡´æ•°æ®å¼‚å¸¸å—
> 
> å¹‚æ¬¡ç‰¹å¾æ˜¯ä¸ºäº†è®©çº¿æ€§æ¨¡å‹æ‹Ÿåˆæ›²çº¿å…³ç³»ï¼Œè™½ç„¶æ•°å€¼å¯èƒ½å˜å¤§ï¼Œä½†é€šè¿‡ä¼˜åŒ–ä»£ä»·å‡½æ•°å¯¹åº”çš„ $\theta$ä¼šæŠµæ¶ˆè¿™ç§æ”¾å¤§æ•ˆåº”ï¼Œä¿è¯æ¨¡å‹è¾“å‡ºåˆç†ã€‚

---

## 4.æ­£è§„æ–¹ç¨‹ Normal Equations

### 1ï¸.èƒŒæ™¯

åœ¨çº¿æ€§å›å½’ä¸­ï¼Œæˆ‘ä»¬å¸Œæœ›æ‰¾åˆ°æœ€ä¼˜å‚æ•° $\theta$ï¼Œä½¿ä»£ä»·å‡½æ•°æœ€å°ï¼š

$$
J(\theta) = \frac{1}{2m} \sum_{i=1}^m (h_\theta(x^{(i)}) - y^{(i)})^2
$$

- **æ¢¯åº¦ä¸‹é™**æ–¹æ³•éœ€è¦è¿­ä»£æ›´æ–° $\theta$ï¼Œé€‰æ‹©å­¦ä¹ ç‡ $\alpha$ï¼Œæ”¶æ•›é€Ÿåº¦å—ç‰¹å¾å°ºåº¦å½±å“ã€‚  
- **æ­£è§„æ–¹ç¨‹**æä¾›ä¸€ç§**ç›´æ¥æ±‚è§£æœ€ä¼˜ $\theta$ çš„æ–¹æ³•**ï¼Œæ— éœ€è¿­ä»£ã€‚

### 2ï¸.æ­£è§„æ–¹ç¨‹å…¬å¼

å‡è®¾ï¼š

- $X$ æ˜¯ç‰¹å¾çŸ©é˜µï¼ˆæ¯è¡Œæ˜¯ä¸€ä¸ªæ ·æœ¬ï¼Œæ¯åˆ—æ˜¯ä¸€ä¸ªç‰¹å¾ï¼Œç¬¬ä¸€åˆ—å…¨æ˜¯ 1 å¯¹åº” $\theta_0$ï¼‰  
- $y$ æ˜¯ç›®æ ‡å‘é‡  

**å…¬å¼**ï¼š

$$
\theta = (X^T X)^{-1} X^T y
$$

- $X^T$ â†’ çŸ©é˜µè½¬ç½®  
- $(X^T X)^{-1}$ â†’ é€†çŸ©é˜µ  
- è®¡ç®—ç»“æœ $\theta$ å°±æ˜¯æœ€å°åŒ–ä»£ä»·å‡½æ•°çš„å‚æ•°

### 3ï¸.ä¸ºä»€ä¹ˆè¦æ­£è§„æ–¹ç¨‹

1. **ç›´æ¥æ±‚è§£æè§£**ï¼šä¸éœ€è¦è¿­ä»£ï¼Œæ¢¯åº¦ä¸‹é™å¯èƒ½éœ€è¦å¾ˆå¤šæ­¥æ‰èƒ½æ”¶æ•›ã€‚  
2. **å¯¹ç‰¹å¾å°ºåº¦ä¸æ•æ„Ÿ**ï¼šä¸å¿…åšç‰¹å¾ç¼©æ”¾ã€‚  
3. **é€‚åˆå°è§„æ¨¡é—®é¢˜**ï¼šå½“ç‰¹å¾æ•°é‡ä¸å¤šæ—¶ï¼Œæ±‚é€†çŸ©é˜µå¼€é”€å¯ä»¥æ¥å—ã€‚

### 4ï¸.æ­£è§„æ–¹ç¨‹çš„æ¨å¯¼æ€è·¯

### çº¿æ€§å›å½’é¢„æµ‹

å‡è®¾æœ‰ $m$ ä¸ªæ ·æœ¬ï¼Œ$n$ ä¸ªç‰¹å¾ï¼š

$$
h_\theta(X) = X \theta
$$

å…¶ä¸­ï¼š

- $X$ï¼š$m \times (n+1)$ çš„ç‰¹å¾çŸ©é˜µ  

$$
X = 
\begin{bmatrix}
1 & x_1^{(1)} & x_2^{(1)} & \dots & x_n^{(1)} \\
1 & x_1^{(2)} & x_2^{(2)} & \dots & x_n^{(2)} \\
\vdots & \vdots & \vdots & \ddots & \vdots \\
1 & x_1^{(m)} & x_2^{(m)} & \dots & x_n^{(m)} \\
\end{bmatrix}_{m \times (n+1)}
$$
- $\theta$ï¼š$(n+1) \times 1$ çš„å‚æ•°å‘é‡  

$$
\theta = 
\begin{bmatrix}
\theta_0 \\
\theta_1 \\
\theta_2 \\
\vdots \\
\theta_n
\end{bmatrix}_{(n+1) \times 1}
$$

- $y$ï¼šçœŸå®çš„ç›®æ ‡å€¼å‘é‡ï¼Œ$m \times 1$ çš„ç›®æ ‡å‘é‡

$$
y = 
\begin{bmatrix}
y^{(1)} \\
y^{(2)} \\
y^{(3)} \\
\vdots \\
y^{(m)}
\end{bmatrix}_{m \times 1}
$$

- $h_\theta(X)$ï¼šæ¨¡å‹çš„é¢„æµ‹å€¼

å¦‚æ­¤å¾—åˆ°è¯¯å·®å‘é‡ $e$ = $ h_\theta(X) $ - $y$

### ä»£ä»·å‡½æ•°ï¼ˆå‘é‡å½¢å¼ï¼‰

$$
J(\theta) = \frac{1}{2m} e^T e
= \frac{1}{2m} (h_\theta(X) - y)^T (h_\theta(X) - y)
$$

- å› ä¸º $h_\theta(X) = X \theta$ï¼Œæ‰€ä»¥å¯ä»¥å†™æˆï¼š

$$
J(\theta) = \frac{1}{2m} (X \theta - y)^T (X \theta - y)
$$

æœ€å°å€¼æ¡ä»¶

- ä»£ä»·å‡½æ•°æ˜¯å‡¸å‡½æ•°ï¼ˆäºŒæ¬¡å½¢å¼ï¼‰ï¼Œæœ€å°ç‚¹æ»¡è¶³æ¢¯åº¦ä¸º 0  

$$
\frac{\partial J(\theta)}{\partial \theta} = 0
$$

å‘é‡åå¯¼

å¯¹ä»£ä»·å‡½æ•°æ±‚å¯¼å¾—åˆ°ï¼š

$$
\frac{\partial J(\theta)}{\partial \theta} = \frac{1}{m} X^T (X \theta - y)
$$

- ä»¤å¯¼æ•°ä¸º 0ï¼š

$$
X^T (X \theta - y) = 0
$$

è§£å‡º $\theta$ï¼Œæ•´ç†æ–¹ç¨‹ï¼š

$$
X^T X \theta = X^T y
$$

å‡è®¾ $X^T X$ å¯é€†ï¼š

$$
\theta = (X^T X)^{-1} X^T y
$$

> è¿™é‡Œä¸ºä»€ä¹ˆä¼šå‡è®¾å¯é€†ï¼Ÿ
>
> - æ­£è§„æ–¹ç¨‹çš„è§£éœ€è¦æ±‚é€†çŸ©é˜µ $(X^T X)^{-1}$  
> - å¦‚æœ $X^T X$ ä¸å¯é€†ï¼ˆå¥‡å¼‚çŸ©é˜µï¼‰ï¼Œå°±æ— æ³•ç›´æ¥æ±‚é€†  
> - ä¸å¯é€†çš„åŸå› é€šå¸¸æ˜¯ï¼š
>  1. ç‰¹å¾ä¹‹é—´**å®Œå…¨çº¿æ€§ç›¸å…³**ï¼ˆå¤šé‡å…±çº¿æ€§ï¼‰  
>  2. ç‰¹å¾æ•° $n+1$ å¤§äºæ ·æœ¬æ•° $m$  
>
>- è§£å†³æ–¹æ³•ï¼š
>  - ç§»é™¤å†—ä½™ç‰¹å¾  
>  - ä½¿ç”¨**æ­£åˆ™åŒ–**ï¼ˆå¦‚å²­å›å½’ï¼‰ä¿è¯çŸ©é˜µå¯é€†

### æ­£è§„æ–¹ç¨‹æœ€ç»ˆå½¢å¼

æ­£è§„æ–¹ç¨‹ç›´æ¥ç»™å‡ºæœ€ä¼˜å‚æ•° $\theta$ï¼š

> $$
\theta = (X^T X)^{-1} X^T y
$$

- $X$ï¼š$m \times (n+1)$ ç‰¹å¾çŸ©é˜µï¼ˆç¬¬ä¸€åˆ—å…¨ä¸º 1ï¼‰  
- $y$ï¼š$m \times 1$ ç›®æ ‡å‘é‡  
- $\theta$ï¼š$(n+1) \times 1$ æœ€ä¼˜å‚æ•°å‘é‡  

> ç›´è§‚ç†è§£ï¼šé€šè¿‡çŸ©é˜µè¿ç®—ç›´æ¥æ‰¾åˆ°æœ€å°è¯¯å·®ç‚¹ï¼Œä¸éœ€è¦è¿­ä»£ã€‚


## 4. [Python ç¤ºä¾‹](./example/trainingMutiULR.py)

é‡‡ç”¨grokç”Ÿæˆçš„[æ•°æ®é›†](./dateSet/priceBymultiLinear-dataset.csv)ï¼Œç±»ä¼¼è¿™æ ·;

|Size_sqm|floor|age_years|Distance_km| Price_wan |
|-----------   |---|--------|-------|-----------|
|106.18|7|26.5|1.5| 677784.0  |
|192.61|17|12.1|3.0| 1224394.0 |
|159.8|20|4.7|0.25| 1087393.0 |
|139.8|4|44.9|0.28| 753724.0  |
|73.4|5|45.0|4.13| 470202.0  |
|73.4|7|31.7|1.86| 434783.0  |

- **ç›®æ ‡**ï¼šåˆ©ç”¨å¤šå˜é‡ç‰¹å¾ `Size_sqm`ã€`floor`ã€`age_years`ã€`Distance_km` æ¥é¢„æµ‹æˆ¿ä»· `Price_wan`  
- **æ–¹æ³•**ï¼š
  1. è¯»å– CSV æ•°æ®é›†  
  2. æ„å»ºç‰¹å¾çŸ©é˜µ `X` å’Œç›®æ ‡å‘é‡ `y`  
  3. ä½¿ç”¨ `scikit-learn` çš„ `LinearRegression` æˆ–è‡ªå®šä¹‰æ­£è§„æ–¹ç¨‹è®­ç»ƒæ¨¡å‹  
  4. è¾“å‡ºæ¨¡å‹å‚æ•°å’Œé¢„æµ‹ç»“æœ  

æˆ‘æ‰“ç®—åœ¨ä»£ç é‡Œå®ç°æ¢¯åº¦ä¸‹é™å’Œæ­£è§„æ–¹ç¨‹ä¸¤ç§æ–¹å¼æ¥æœ€å¯¹æ¯”ï¼Œ
ä½†æ˜¯ä¸€å¼€å§‹æ¢¯åº¦ä¸‹é™è¿™æ ·å†™çš„æ–¹æ³•
```python
def gradient_descent(X, y, lr=0.01, n_iter=1000):
    m, n = X.shape
    theta = np.zeros((n, 1))
    for iteration in range(n_iter):
        gradient = (1/m) * X.T @ (X @ theta - y)
        theta -= lr * gradient
    return theta
```
é‡Œé¢çš„lrä¾¿æ˜¯$\alpha$,ç”±äºæˆ‘çš„çš„å›ºå®šå–å€¼ä¾¿å¯¼è‡´äº†è¿™æ ·çš„è¾“å‡ºï¼Œ
```
æ¢¯åº¦ä¸‹é™ theta: [nan nan nan nan nan]
```
è¿™æ˜¯å¯èƒ½çš„åŸå› 
- 1.å­¦ä¹ ç‡å¤ªå¤§
- 2.æ•°æ®æ²¡æœ‰æ ‡å‡†åŒ–/å½’ä¸€åŒ–
- 3.è¿­ä»£æ¬¡æ•°å¤šã€ç´¯è®¡è¯¯å·®

å¦‚æœå­¦ä¹ ç‡è¿‡å¤§ + è¿­ä»£æ¬¡æ•°é«˜ï¼Œå¾ˆå¿«å°±æº¢å‡ºäº†ã€‚

æ‰€ä»¥è¿™é‡Œè¦å¯¹$\alpha$åˆå§‹å€¼è°ƒæ•´ï¼Œç„¶åéšè¿­ä»£æ¬¡æ•°å‡å°ï¼Œ
å¸¸è§çš„è°ƒæ•´æ–¹æ³•æœ‰ï¼š
- å›ºå®šè¡°å‡ï¼ˆéšè¿­ä»£æ¬¡æ•° 1/k ç¼©å°ï¼‰:lr = lr0 / (1 + decay * iteration)
- æŒ‡æ•°è¡°å‡ï¼šlr = lr0 * (decay ** iteration)
- åˆ†æ®µè¡°å‡ï¼š æ¯éš” N æ­¥æŠŠ lr å‡å°ä¸€åŠã€‚
æ”¹è¿›çš„ä»£ç ï¼š
```python
def gradient_descent(X, y, lr0=0.1, decay=0.001, n_iter=1000):
    m, n = X.shape
    theta = np.zeros((n, 1))
    for iteration in range(n_iter):
        # å­¦ä¹ ç‡éšè¿­ä»£æ¬¡æ•°å‡å°
        lr = lr0 / (1 + decay * iteration)
        gradient = (1/m) * X.T @ (X @ theta - y)
        theta -= lr * gradient
    return theta
```
æœ€ç»ˆè·‘ä¸‹æ¥çœ‹ä¸€ä¸‹ï¼Œæˆ‘å‡†å¤‡çš„è¿™æ ·çš„å°æ¨¡å‹ï¼Œè¿™æ ·ä¸¤ä¸ªæ–¹æ³•ç”¨æ—¶å¯¹æ¯”å·®è·å°±å¾ˆå¤§äº†
```
=== æ¨¡å‹å‚æ•°å¯¹æ¯” ===
æ­£è§„æ–¹ç¨‹ theta: [104126.0251   4974.0859   9852.9724  -2028.9417  -2316.3184]
æ¢¯åº¦ä¸‹é™ theta: [743186.75   220849.0929  61946.6021 -29235.5483  -3299.1717]

=== ç”¨æ—¶å¯¹æ¯” ===
æ­£è§„æ–¹ç¨‹ç”¨æ—¶: 0.000060 ç§’
æ¢¯åº¦ä¸‹é™ç”¨æ—¶: 0.039314 ç§’
```

æœ¬æ¥æƒ³è¾“å‡ºä¸€ä¸ªå­¦ä¹ ç‡å˜åŒ–çš„å›¾æ¥ç›´è§‚æ„Ÿå—ä¸€ä¸‹ï¼Œ
ä½†æ˜¯åœ¨ æ•°æ®é‡å°ã€è¿­ä»£æ¬¡æ•°å¤š çš„åœºæ™¯ä¸‹ï¼Œdecay=0.001 å…¶å®ä¸‹é™å¾—å¤ªå¿«äº†ï¼Œ
æŠŠ decay è°ƒå°äº†å¾ˆå¤šï¼ˆä»åŸæ¥çš„ 0.001 â†’ 0.0001ï¼‰ï¼Œä¸€äº›ä»£ç ä¿®æ”¹åã€‚
ç°åœ¨è¾“å‡ºçš„å­¦ä¹ ç‡æ›²çº¿å°±å¯ä»¥å¾ˆæ˜æ˜¾çš„çœ‹å‡ºæ¥äº†ï¼Œä½†æ˜¯ä»£ä»·å‡½æ•°ä¸‹é™çš„ä¾æ—§å¤ªå¿«äº†ï¼Œ
<div align="center">
    <img src="./images/example_UnivariateMutiUlR.png" alt="å›¾ç¤º" style="width: 90%; max-width: 600px; min-width: 300px;">
</div>

[ä¸Šä¸€ç« ](./LinearRegressionWithOneVariable.md) --- [ä¸‹ä¸€ç« ]()